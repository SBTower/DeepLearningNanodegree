{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and initialization\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "Now we have some understanding of the maths, in this section we will start to look at some of the key concepts that enable efficient training of neural networks on real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Model Accuracy\n",
    "We know how to train a model, but how do we measure how well our model fits the data? The logical solution is to simply find the model that minimizes (or completely removes) any errors on the data provided. However, this isn't actually true.\n",
    "\n",
    "### Overfitting and Underfitting\n",
    "What tends to happen if we train a model simply to reduce the error in the training set is that the model *overfits* to the particular patterns in the training data which aren't actually representative of the underlying properties of the class boundaries. This can result in a very high accuracy on the training data, but actually a low accuracy on any data found in the real world. this is because the model becomes too *complex*, learning to fit into all of the tiny deviations in order to better fit random noise in the training data. We therefore counter this by both utilising a dedicated test set, and actively trying to reduce the complexity of the model. The aim is to result in a model that *generalizes* well to real world data.\n",
    "\n",
    "<img src=\"images/overfitting.png\" alt=\"drawing\" width=\"550px\"/>\n",
    "\n",
    "#### Underfitting -> High Bias\n",
    "\n",
    "#### Overfitiing -> High Variance\n",
    "\n",
    "This concept is described in the following videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/EeBZpb-PSac?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/EeBZpb-PSac?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xj4PlXMsN-Y?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xj4PlXMsN-Y?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preventing Overfitting\n",
    "Generally we train a model with an architecture that is too complex, and then we use a number of techniques to cause the model to try and reduce its complexity.\n",
    "\n",
    "### Early Stopping\n",
    "Early stopping is quite simple; you stop training the model early to prevent it from becoming too complex as the model continuously updates its weights each epoch. However it can be difficult to know when exactly the right time to stop is. As we continue to train, the accuracy on the training set will continue to decrease, as the model learns more and more how to fit the training data, and its noise. It isn't as simple as minimizing this error, as it is always decreasing. Therefore we use a *testing* (or *validation*) set instead.\n",
    "\n",
    "The validation data set is a set of data which is held separately from the training process, and so the neural network cannot learn how to fit the noise in this training set as it never sees the errors produced. However, we put the data from the validation set through the model (feedforward only, **NO** backprop) and we can measure the accuracy of the model on data it has never seen before. As the model starts to learn, the accuracy on both the training and validation sets start to decrease. However, eventually, as the model starts to overfit the training data, the error on the validation set will start to increase, whilst it will still be decreasing on the training set. We can therefore stop training at the point where we *minimize the error on the validation set*, as this demonstrates a good generalization to previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NnS0FJyVcDQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NnS0FJyVcDQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "Another form of overfitting can occur when the absolute values of the weights gets too large. This is hard to recognise logically, but a simple example helps make it clear.\n",
    "\n",
    "Imagine we have one simple model with small weights. When we calculate the error, we take the sigmoid of the error, which works well for small numbers. If we then take this model and simply apply a scalar transform to the weights so they are much larger (e.g. w1 goes from 1 to 100), then instinctively we know the model should perform just as well as the original model as they describe the same boundary. However when we take the sigmoid of this larger number, the reported error is much much lower. This is bad, as we need to be able to see the errors and apply them effectively to the model weights. What we find when using a model with very large weights is that the model is **too certain**, meaning it only ever returns high confidence of success and very rarely returns an error with a useful gradient to learn from. Therefore we want to truy and avoid the model weights becoming too large, and encourage the model to use lower weights through *regularization*\n",
    "\n",
    "Regularization involves adding an extra term to the error function which is large if the weights are large. This will artificially increase the error if the model weights are too big, and therefore cause the training process to try and find a solution with smaller weights. There are two main methods for doing this, L1 and L2 regularization.\n",
    "\n",
    "<img src=\"images/regularization.png\" alt=\"drawing\" width=\"550px\"/>\n",
    "\n",
    "L1 regularization involves simply taking the sum of the absolute values of the weights, whereas L2 involves taking the sum of the squares. Whilst both of these seem to achieve a similar objective, they actually result in quite different models. L1 regularization tends to result in model weights that are either close to 1 or close to 0, and is therefore very useful for performing feature selection/importance analysis. L2 regularization tends to result in most weights being quite small, and is more often used for training models.\n",
    "\n",
    "Lambda is known as the *regularization parameter*, and controls how much regularization to do. It tends to be held between 0 and 1 (and values larger than 1 can result in the problem failing to converge). A value close to 0 means that there will be a small regularization factor, and a value closer to 1 means that there will be a lot of regularization. This is a hyperparameter that has to be controlled as the training process happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ndYnUrx8xvs?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ndYnUrx8xvs?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "The other techniques are general techniques to prevent overfitting that can be applied to most machine learning algorithms. However dropout is more specific to Neural Networks, and involves the occasional *dropping out* of a node in the network so that the model can learn a more balanced representation, where no one set of connections is dominating the model. This helps to ensure that every part of the model takes part in the training. If we didn't do this, it is possible that some parts of the network are never trained, as the associated weight is tiny compared to the other branches.\n",
    "\n",
    "We implement dropout by defining a percentage chance that each node will be ignored for this particular application of the feedforward/backprop cycle. In each cycle, the probability is applied to each node again, so that there is a different set of nodes being dropped in each cycle. This encourages all parts of the network to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ty6K6YiGdBs?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ty6K6YiGdBs?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Local Minimum\n",
    "Quite often the error plane will be complex, with lots of local sub-optimal minima that can cause our gradient descent algorithm to get stuck. We want to try and avoid this so that the algorithm will find the global minimum that solves the problem as best as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gF_sW_nY-xw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gF_sW_nY-xw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Restarts\n",
    "One way to achieve this is to randomly initialize the solution to different points, as this artificially searches the error space for the global minimum. Whilst this doesn't do anything particularly clever, it does give a better coverage of the solution space and often allows us to easily identify a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/idyBBCzXiqg?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/idyBBCzXiqg?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing Gradient Problem\n",
    "The backpropagation algorithm relies on feeding back the gradients of the error through the layers of the network. However, quite often these gradients can be very small. As we propagate this error through the network, we are taking the products of lots of very small numbers (made even smaller by the sigmoid function). As the depth of the network increases, the gradient can become tiny by the time it reaches some of the initial layers. This makes applying gradient descent very difficult (and very slow!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/W_JJm_5syFw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/W_JJm_5syFw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Activation Function\n",
    "One of the ways to fix the vanishing gradient problem is to use a different activation function. One of the more popular alternatives to the sigmoid function is the **ReLU** (Rectified Linear Unit) activation function:\n",
    "\n",
    "<img src=\"images/relu.png\" alt=\"drawing\" width=\"550px\"/>\n",
    "\n",
    "Or the **Tanh** activation function:\n",
    "\n",
    "<img src=\"images/tanh_v_sigmoid.jpg\" alt=\"drawing\" width=\"550px\"/>\n",
    "\n",
    "Both of these functions result in larger gradients, and help to reduce the impact of the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kA-1vUt6cvQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kA-1vUt6cvQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "This is a method that helps to speed up the learning and also leads to better solutions. Instead of running each training step on all of the training data, we instead split our training set into *batches*, each containing a small subset of the data. Each training iteration will now only run on one of these batches, and then the next iteration will run on a different batch. Over time we still end up using all of the training data, but we take more, faster, but less accurate steps towards the goal, rather than taking well judged but much slower steps. in practice, this can result in faster training and a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2p58rVgqsgo?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/2p58rVgqsgo?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Learning Rate\n",
    "It can be difficult to know what learning rate to use when training the model, and choosing this value is one of the main jobs a practitioner has to do. In general, we can say that choosing a large learning rate results in larger steps in each gradient descent iteration. This can lead to a solution which jumps wildly around the solution space and may even fail to converge at all. Alternatively, if the learning rate is too small, the gradient descent steps will be very small, and the solution will take a very long time to converge to a solution. So in reality, we need to find the right value that is quick enough to converge, but doesn't result in exploding gradients.\n",
    "\n",
    "Generally, a good thing to try if the model is not training initially is to decrease the learning rate. Additionally there are some approaches which alter the learning rate as the solution moves, where a large learning rate is used on areas with a larger gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/TwJ8aSZoh2U?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/TwJ8aSZoh2U?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "This is a method used to help the solution *power through* local minimum. Generally we have the problem with local minima that the gradient at that point is zero, and so the solution has converged and doesn't move again. Momentum involves using the gradient of the last few steps to derive the current direction to move, and so this allows us to power through particular local minima and potentially find better solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/r-rYz_PEWC8?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/r-rYz_PEWC8?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NanoDL",
   "language": "python",
   "name": "nanodl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
