{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and initialization\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Sentiment Analysis is a particularly interesting application of deep learning in a field known as *Natural Language Processing*, or NLP. In Sentiment analysis we want to take as input to our network a passage of text, and output the sentiment of the words. This can be things such as positive or negative, or more specific, like happy, confused, or angry.\n",
    "\n",
    "The primary problem posed here is the issue that the input to the network is words. This isn't really what Neural Networks handle, so before we can pass our data through a Neural Network we must first transform our textual data into a format that the Neural Network can understand; numbers.\n",
    "\n",
    "This video introduces the problem and describes what we hope to achieve in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/da1I0mea1jQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/da1I0mea1jQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go and create a Neural Network, we need to validate our theory of what data representation will provide a *signal* that accurately correlates between the input data and the output sentiment. When taking text as input, the most common way of representing the data to generate this signal is to use a *bag-of-words* representation, where each review is represented as a count of the number of times each word is used. This should help us identify sentiment, as words such as 'terrible', 'fantastic', 'awful' and 'excellent' will likely be more common in different sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IsTOnkAKaJw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/IsTOnkAKaJw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the lab work can be found here, and this contains many of the important notes for the lesson:\n",
    "\n",
    "Link to [Sentiment Analysis Lab](Labs/sentiment-analysis/Sentiment_Classification_Projects.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Verifying the signal\n",
    "Initially our intuition said that the count of each word would give us a good correlation between the input and the output sentiment. However, if we were to simply take a raw count of all of the words, the signal gets lost in a lot of noise, as there are a very large number of neutral words that occur in large quantities in both positive and negative samples (words such as `a`, `the`, `.`, `in`, `and` etc.). So instead, a better representation is:\n",
    "\n",
    "    `log(positive_count / negative_count)`\n",
    "    \n",
    "By taking the ratio of the count of words in the positive reviews and negative reviews, we can see which words are occurring more often in the positive text rather than the negative. Taking the log of this ratio centers the scale, so that neutral words are approximately 0, negative words have a negative magnitude and positive words a positive magnitude. This allows us to easily identify which words contribute to the signal that the neural network will try and learn.\n",
    "\n",
    "Note that this is only used for verifying the signal exists in the representation of a paragraph as a count of individual words. We will next use this knowledge to define our method for converting the paragraph into a numerical input ready for a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/l4r5l0HvHRI?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/l4r5l0HvHRI?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Transforming the Input Data\n",
    "Now that we have identified the pattern, we need to define our input data conversion and our expected output data format. This is relatively simple; we will just count the number of times each word occurs in our review and use this as input to the network. The output will be a single binary output, where 0 represents negative and 1 represents positive.\n",
    "\n",
    "See the associated lab for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7rHBU5cbePE?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7rHBU5cbePE?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/45ihpPaeO8E?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/45ihpPaeO8E?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Building a Neural Network\n",
    "Now we have our data in a format we can use as input to the Neural Network, it is time to build the architecture. Again, see the lab for the full implementation. Using a simple network with one input node for every word in our vocabulary (~ 74000), 10 hidden units and 1 output node we are able to show that the network starts to learn something that allows us to predict sentiment (though not very well). However, there are other techniques we can employ to start improving the performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/imnxzCev4SI?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/imnxzCev4SI?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Understanding Neural Noise\n",
    "In our Neural Network, we noticed that it took a long time to train, and the final accuracy was still quite poor. When this happens, we need to think about what the signal is in our data and what is causing noise on that signal that is making it difficult for the network to learn the correct patterns. By reducing this signal-to-noise ratio by tweaking the network architecture, or the network parameters, or the data format, or any number of other things, we can drastically improve the networks performance.\n",
    "\n",
    "One of the major sources of noise in the current methodology is the prominance of filler words in the reviews. We initially chose to use the counts of each word in the review as the input to the network. However in most reviews, there is a large number of irrelevant words such as `the`, `a` and `.` which dominates the signal, making it very hard for the neural net to learn to find the signal from the important words like `brilliant` or `offensive`. To remove this noise, we can instead not use the *counts* of each word in the review, but just have a binary switch where if a word is present in the review, the node takes a value of 1, otherwise it takes a value of 0. This drastically reduces the noise, and also drastically improves the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ubqhh4Iv7O4?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ubqhh4Iv7O4?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Analyzing Inefficiencies in Our Network\n",
    "Another source of inefficiency is in the actual computational expense required to do a forward pass and backprop pass on a network with 74000 input nodes. Because our input data is very sparse (meaning that the majority of the input values are 0), we are actually spending a lot of computational effort performing operations that make no difference to the actual training process (as it involves summing something multiplied by 0). Doing this tens of thousands of times per training step seems very inefficient, and so we can tweak our training process to skip these unnecessary operations and speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4MuS-6ATxCU?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4MuS-6ATxCU?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Hv86B_jjWTI?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Hv86B_jjWTI?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to reduce the noise and complexity of the system is to reduce the vocabulary and only focus on the words that actually provide a signal. We actually did this earlier when we were validating the correlation between the words and the sentiment by calculating the ratio between number of times the word appears in a positive review compared to a negative review. By removing the words that are generally neutral, we can drastically reduce the number of inputs to the network and make the signal much more obvious to the neural network. It is also common to remove the words that have a very low frequency, as it is hard to learn a general pattern from something that is only seen once or twice. Also, it is common to remove the *most* frequent words, as these tend to be the filer words such as `the` or `and`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Kl3hWxizKVg?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Kl3hWxizKVg?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ji0famK7gOQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ji0famK7gOQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Learning\n",
    "As the network learns to identify a positive or negative sentiment from the input words, it is actually also learning a vector space where negative words are clustered together, and positive words are also clustered together. By looking at the weights of the input -> hidden part of the network, we can start to see that negative words have a similar vector when compared to each other, and similarly with the positive words. This can be visualised by performing a dimensionality reduction and plotting the vector space of the positive and negative words, as shown in this video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UHsT35pbpcE?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UHsT35pbpcE?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NanoDL",
   "language": "python",
   "name": "nanodl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
