{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and initialization\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep Learning? What is it used for?\n",
    "\n",
    "For the second question, deep learning is used pretty much everywhere. Detecting spam emails, playing games, diagnosing diseases and self-driving cars. At the center of all of these applications is a machine learning technique called *Neural Networks*\n",
    "\n",
    "Neural networks are essentially *function approximators*, where they can accurately approximate any function, taking a number of inputs, performing a series of simple operations on them, and delivering a number of outputs. A Neural Network generally looks like the following:\n",
    "\n",
    "<img src=\"images/deep_neural_network.png\" alt=\"drawing\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Problems\n",
    "\n",
    "Classification problems are a general type of machine learning problem. It involves determining which *class* a particular entity belongs to, using a model to automatically decide which is the best fit. Generally these are solved by training a model from historical *labelled* data sets, allowing boundaries between the classes to be defined that bets seperated the *training data* and using these trained boundaries on the *test data*\n",
    "\n",
    "### An example: Determining if a student will be accepted to university\n",
    "\n",
    "Students apply to a university with two scores, their test result and their school grades. Based on these, can we find which students will get accepted and which will get rejected? This problem is shown in the following video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Dh625piH7Z0?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Dh625piH7Z0?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example in the video above asks us to determine if a student with a test score of 7 and a school grade of 6 would get accepted. By plotting this point on the chart with all of the other data, it appears to show that the student should be accepted as they in an area of the data where other students have previously been accepted. This logic is what the machine learning algorithms aim to replicate, finding regions in the feature space where students are accepted, and other regions where they aren't.\n",
    "\n",
    "A simple machine learning algorithm might try and draw a straight line between the two *classes* of accepted and rejected students based on the data. The following video shows what this might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/46PywnGa_cQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/46PywnGa_cQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution seems to work most of the time, and is a good approximation of the *hyperplane* that seperates the data. But how do we find this line from the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Boundaries\n",
    "\n",
    "Our features in this example are defined as\n",
    "`x1` = test score\n",
    "`x2` = school grades\n",
    "\n",
    "We can define a linear boundary in this feature space that splits the feature space into two, one space for each *class*, accept or reject.\n",
    "\n",
    "The line shown in the video has the equation:\n",
    "`2x1 + x2 = 18`\n",
    "\n",
    "Using this, it is possible to define a boundary where the *score* of a new student can be calculated as:\n",
    "`2x1 + x2 - 18`\n",
    "If this score is bigger than 0, then the student will be accepted. Otherwise, they will be rejected.\n",
    "\n",
    "In general, the equation for the linear boundary is:\n",
    "`w1*x1 + w2*x2 + b = 0`\n",
    "where `w1` and `w2` are the *weights* and `b` is the *bias*\n",
    "\n",
    "In matrix/vector notation, this can be written as:\n",
    "`Wx + b = 0`,\n",
    "where W is the vector of weights and x is the vector of input features.\n",
    "\n",
    "Each data point is associated with a label `y`, and in this binary classification example, `y` can be either 0 or 1.\n",
    "\n",
    "All of this is explained in the following video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/X-uMlsBi07k?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/X-uMlsBi07k?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher dimensions\n",
    "\n",
    "The above equations all hold for data sets with larger dimensions. For example, if we now have an extra feature for each student (e.g. class rank), our weight and input vectors will now simply be of length 3 instead of length 2, and the hyperplane seperating the two classes becomes a *plane*.\n",
    "\n",
    "As we add more and more dimensions to the input data, this logic still holds, as we extend our weights and input vectors to have length `n`. Whilst this is hard to imagine graphically, the equation `Wx + b = 0` still defines a linear boundary (with dimension `n-1`) separating the feature space into two distinct spaces.\n",
    "\n",
    "This is shown in the following video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/eBHunImDmWw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/eBHunImDmWw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrons\n",
    "\n",
    "These are the building blocks of Neural Networks, and take the following form:\n",
    "\n",
    "<img src=\"images/Single-Perceptron.png\" alt=\"drawing\" width=\"600px\"/>\n",
    "\n",
    "It is essentially an encoding of the above equation into a *node*. Within each node, firstly the `score` is calculated by multiplying the weights with each corresponding input, as well as a *bias node* (not shown in the diagram). This sum is essentially our equation `Wx + b`\n",
    "\n",
    "The result of this sum is then passed through an *activation function* that transforms the result into a weighting. There are numerous activation functions that generally scale the output to be between -1 and 1. In our previous example, we were using a *step function* for the activation function, where if the score was less than 0, the activation function would output 0. If the score was greater than 0, then the activation function would output 1. These outputs are our labels for the class.\n",
    "\n",
    "All of this is well explained in the following video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hImSxZyRiOw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hImSxZyRiOw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can understand this as the perceptron taking as input the feature values, and returning the *probability* that the feature belongs to a particular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/07-JJ-aGEfM?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/07-JJ-aGEfM?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Perceptron from data\n",
    "\n",
    "We want to be able to find the weights and bias of the equation automatically based on the data. To do so, we need to adjust these in order to minimize the number of points that are misclassified by the equation. A neat way to think about this is by thinking that all the points that are classified correctly are telling the boundary line to move further away, whilst all of the misclassified points are telling the line to get closer. This is described in the following videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-zhTROHtscQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-zhTROHtscQ?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/fATmrG2hQzI?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/fATmrG2hQzI?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/lif_qPmXvWA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/lif_qPmXvWA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/p8Q3yu9YqYk?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/p8Q3yu9YqYk?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write some code that will perform the Perceptron algorithm on a set of data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Fill in code\n",
    "    for index in range(len(X)):\n",
    "        pred = prediction(X[index], W, b)\n",
    "        if y[index] == 0 and pred == 1:\n",
    "            W[0] = W[0] - learn_rate * X[index][0]\n",
    "            W[1] = W[1] - learn_rate * X[index][1]\n",
    "            b = b - learn_rate\n",
    "        elif y[index] == 1 and pred == 0:\n",
    "            W[0] = W[0] + learn_rate * X[index][0]\n",
    "            W[1] = W[1] + learn_rate * X[index][1]\n",
    "            b = b + learn_rate\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear regions\n",
    "\n",
    "Everything we have seen so far looks at using the perceptron algorithm to find a straight line, or *linear boundary* that accurately seperates the two classes. However this might not always be possible, we might instead need a *non-linear* boundary, such as a circle or a curve. Fortunately, with a couple of simple adjustments, the perceptron algorithm can be altered to find such hyperplanes.\n",
    "\n",
    "An example is described in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/B8UrWnHh1Wc?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/B8UrWnHh1Wc?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error functions\n",
    "\n",
    "One important concept to understand before continuing any further is the *error function*. This is a function which describes how well a particular solution meets the objective of classifying the classes. There are multiple different error functions, and which one to use will depend on both the algorithm and the data. For instance, a different error function will be chosen if the data is continuous or discrete.\n",
    "\n",
    "A common error function is the *least-squares distance* metric, where this calculates the sum of the square error between the actual value and the predicted value. For example, in our perceptron example earlier, we would mark the error as the sum of the *distance* between the point and the line.\n",
    "\n",
    "Once we have a measure of the *error* in the current solution, we can define an algorithm that aims to adjust the solution so as to minimize the total error. This algorithm is generally *gradient-descent* (or more recently *stochastic gradient descent*), and requires a continuous and differentiable error function.\n",
    "\n",
    "The following video gives a nice analogy for understanding the error function, and introduces some of the concepts of gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jfKShxGAbok?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jfKShxGAbok?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use gradient descent and continuous error functions, we need to change our predictions in our example from discrete values to continous predictions.\n",
    "\n",
    "Before, our discrete prediction simply returned a 0 (for reject) or a 1 (for accept). The most logical way to convert this to a continuous space is to instead take a *probability* as the prediction, as a value between 0 and 1. The output is a function of how close a point is to the hyperplane, where a point which is a long way from the line will be given a high probability of being in that class. Points that are closer to the line will be given a probability closer to 0.5.\n",
    "\n",
    "The easiest way to make this transformation is to choose a continuous activation function. For example, instead of using the *step-function* we used earlier, we can instead use the *sigmoid function*. This returns a value between 0 and 1 describing the probability the data point belongs to class 1.\n",
    "\n",
    "This is described in the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Rm2KxFaPiJg?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Rm2KxFaPiJg?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification\n",
    "\n",
    "In the above examples, we can easily return a single value that determines whether a data point belongs to a class or not. This is known as a *binary classification* problem. However, what can we do if we have multiple classes? For example, if we want to classify the colour in an image, or the type of animal from a picture?\n",
    "\n",
    "Firstly, we calculate a score for each of the possible classes. Once we have these scores, we need a method to convert them into a positive value between 0 and 1 that accurately represents the probability they belong to the class. One way to do this is with the *Softmax* function.\n",
    "\n",
    "The Softmax function has nice properties in that it is increasing, continuous, and ensures the sum of the output is equal to 1. The formula for the softmax function is:\n",
    "\n",
    "<img src=\"images/softmaxequation.jpg\" alt=\"drawing\" width=\"200px\"/>\n",
    "\n",
    "This can be coded in python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.004730360796160469,\n",
       " 0.03495290129101196,\n",
       " 0.2582689484596729,\n",
       " 0.7020477894531546]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    sm_vals = []\n",
    "    for value in L:\n",
    "        sm = np.exp(value)/np.sum([np.exp(x) for x in L])\n",
    "        sm_vals.append(sm)\n",
    "    return sm_vals\n",
    "\n",
    "ans = softmax([1,3,5,6])\n",
    "display(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice description of why we use Softmax is given in the following videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/RC_A9Tu99y4?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/RC_A9Tu99y4?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/n8S-v_LCTms?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/n8S-v_LCTms?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "In multi-class problems, we also need a method to define the labels of each data point. In previous examples, it has been enough to encode 'accept' as 1 and 'reject' as 0, but if we instead need to encode a number of possible discrete labels, then we need a different method. It is not enough to use, say, `0,1,2` as the labels, as this implies an order and a relative scale between the discrete classes. Instead, we use something called *One-Hot Encoding*\n",
    "\n",
    "One-Hot encoding involves giving each data point a label that is a vector with a length equal to the total number of classes, where one value is a 1 and all the others are zeros. The location of the 1 describes which class that data point belongs to.\n",
    "\n",
    "This is described in the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AePvjhyvsBo?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AePvjhyvsBo?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "\n",
    "We now need a method for determining how accurate a model is based on the predicted labels and the actual labels of the data. One popular method for this is the *Maximum Likelihood* method, that aims to calculate the overall probability that the model is correct given the probability of the labels on each data point. Once we have this overall probability metric, we can aim to maximise this value by adjusting our model parameters.\n",
    "\n",
    "This is explained in the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6nUUeQ9AeUA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6nUUeQ9AeUA?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were able to maximise the probability, this would lead to an accurate model. However, raw probabilities involve a large number of *products*, especially as the number of data points increases. This can be very difficult computationally, leading to very negative numbers and expensive computations. Therefore, we use the log function to convert the *products* into *sums* to make the calculations easier. This is called the **cross-entropy**. As we are interested in the *negative logarithm of the probability*, a high cross-entropy indicates a bad model, whereas a low cross-entropy indicates a good model. Therefore our objective goes from maximising a probability to *minimizing the cross-entropy*.\n",
    "\n",
    "The original probability model:\n",
    "\n",
    "`P(x1=1) x P(x2=2) x P(x3=1) x ... x P(xn=2)`\n",
    "\n",
    "The cross-entropy model:\n",
    "\n",
    "`-ln(P(x1=1)) - ln(P(x2=2)) - ln(P(x3=1)) - ... - ln(P(xn=2))`\n",
    "\n",
    "\n",
    "Cross-entropy is explained in the following videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/iREoPUrpXvE?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/iREoPUrpXvE?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1BnhC6e0TFw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1BnhC6e0TFw?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.036554268074246"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8393296907380268"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n",
    "\n",
    "display(cross_entropy([1,1,1],[0.8,0.6,0.1]))\n",
    "display(cross_entropy([1,1,0],[0.8,0.6,0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy can easily be extended to multi-class problems, as the calculation relies on the class and probabilities being known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "We are now ready to put into practice all of the previous techniques and create one complete machine learning algorithm, *logistic regression*. This is a popular classification algorithm, and basically follows these steps:\n",
    "\n",
    "* Take your data\n",
    "* Pick a random model (randomly initialize model parameters)\n",
    "* Calculate the error (cross-entropy) of the model on the data\n",
    "* Minimize the error by adjusting the model parameters\n",
    "* Repeat until a suitable model has been found\n",
    "\n",
    "### Logistic regression error function\n",
    "\n",
    "The logistic regression function uses an error function which is:\n",
    "\n",
    "`Error = - (1 - y)(ln(1 - prediction)) - y(ln(prediction))`\n",
    "\n",
    "This works because if `y=1` only the second half of the equation is calculated, and gives an error if the prediction is 0 (i.e. not 1). Similarly if y=0 and the prediction is 1. We then take the total error to be the sum of all of the errors for each data point, divided by the total number of data points.\n",
    "\n",
    "Remember that the prediction is `sig(Wx + b)`, and so the error function is tied to the model parameters.\n",
    "\n",
    "The objective is to minimize the error by adjusting the model parameters. This can be done by using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V5kkHldUlVU?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/V5kkHldUlVU?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KayqiYijlzc?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KayqiYijlzc?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "This works by surveying the immediate area surrounding the current model, and seeing in which direction is the largest negative gradient taken against the error function, and taking a small step in that direction. By making lots of these small steps, the idea is that the model will eventually end up at a point that minimizes the error and accurately labels the data. In order to calculate the gradient of the error functions, we first need to be able to find its *derivative*.\n",
    "\n",
    "At each step, we will update the value of each weight as:\n",
    "\n",
    "`wi <-- wi - alpha * dE/dwi`\n",
    "\n",
    "`b <-- b - alpha * dE/db`\n",
    "\n",
    "Here alpha is the learning rate, which is a *hyperparameter* of the algorithm and defines how large each step is in the gradient descent process.\n",
    "\n",
    "This process is shown in the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rhVIF-nigrY?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rhVIF-nigrY?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is calculate the formula for the gradient of the error with respect to each weight.\n",
    "\n",
    "We won't go into all the Maths here, but it turns out that for logistic regression the derivative comes out as:\n",
    "\n",
    "`dE/dwj = -(y - y^)xj`\n",
    "\n",
    "`dE/db = -(y - y^)`\n",
    "\n",
    "The gradient descent step then simply becomes:\n",
    "\n",
    "`wi <-- wi + alpha * (y - y^)xi`\n",
    "\n",
    "`b <-- b + alpha * (y - y^)`\n",
    "\n",
    "We now have the complete gradient descent algorithm for logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/snxmBgi_GeU?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/snxmBgi_GeU?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Gradient Descent\n",
    "The Gradient Descent algorithm is implemented in full in the *gradient descent lab*, which can be found here:\n",
    "\n",
    "Link to [Gradient Descent Lab](Labs/gradient-descent/GradientDescent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Data & Models\n",
    "All the examples we have used previously have focused on fitting a linear model to the data space to classify the different classes. However the real world is often much more complex than this, and a linear hyperplane often will perform poorly when trying to seperate the classes. This is where the extension from the single layer perceptron to the more complex Neural Network comes in.\n",
    "\n",
    "Neural Networks are able to define highly complex probability distributions across the feature space, accurately approximating non-linear patterns. Instead of trying to define the hyperplane that seperates the data explicitly, we instead approximate the probability distribution over the entire space using a Neural Network, and then the hyperplane is defined as being anywhere in this space where there is equal probability that the data point belongs to either class (for binary classification problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HWuBKCZsCo8?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HWuBKCZsCo8?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "We mentioned right at the start that a Neural Network is essentially a collection of nodes, each performing a calculation on the data and combining the outputs to result in a *function approximator*. Now we will go into more detail about what each of these nodes is.\n",
    "\n",
    "Essentially, each node is an individual linear perceptron, providing an estimation of the probability distribution over the space using a linear model. By combining multiple instances of these linear perceptrons, we end up with a much more complex, non-linear estimation for the probability distribution.\n",
    "\n",
    "The combination of two linear perceptrons can be imagined as simply adding the two model outputs. So for example, each model will output a *probability* that the data point belongs to a particular class. The combined model will sum these two probabilities, and then apply an *activation function* (such as the sigmoid function) to tranform the output into a value between 0 and 1 that describes the combined probability. The combination can also be easily adjusted by providing a weight that each model provides to the final model, and a bias function to shift the results.  In this way, relatively small networks can be used to approximate a large number of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Boy3zHVrWB4?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Boy3zHVrWB4?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FWN3Sw5fFoM?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FWN3Sw5fFoM?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Layers\n",
    "We can see above how simply combining two linear perceptrons can result in a reasonably complex non-linear model. Additionally more complex models can be created by adding more layers and more nodes to the network, as well as using different activation functions. By adding more nodes and more layers, we begin defining **Deep Neural Networks**, which simply describes a Neural Network with multiple hidden layers.\n",
    "\n",
    "Each Neural Network is made up of *layers*:\n",
    "\n",
    " - The first layer is the **Input Layer**, where the input of the network is defined. This is usually the values of the features attributed to the data point. The number of input nodes describes the dimensionality of the feature space.\n",
    " - The middle layers are the **Hidden Layers**. These are the layers where the linear perceptrons are all combined to form a highly complex non-linear boundary. Additional layers and nodes increase the non-linearity of the model.\n",
    " - The final layer is the **Output Layer**, where the final probability for each class is output. The number of nodes in the output layer corresponds to the number of possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pg99FkXYK0M?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pg99FkXYK0M?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "This is the process used to turn the input of a Neural Network into the output from the network, or alternatively, the process of going from the *feature space* into the classification *prediction*\n",
    "\n",
    "The feedforward process is relatively simple, as all of the equations are simply linear combinations of the weights and biases. The computation can be simplified even further (from a computation point of view) by *vectorizing* the combinations and using matrix algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hVCuvMGOfyY?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hVCuvMGOfyY?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the previous methods using a simple linear perceptron, we want to update the weights (and biases) in our Neural Network to minimize the classification error over the training and validation data sets. Luckily, the same error function as was used then can also be used now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SC1wEW7TtKs?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SC1wEW7TtKs?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our error function, we can accurately determine how accurate our current model is. We now need a method for propogating this error back through the network so we can update the weights to minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "The process by which we propogate the error back through the network is known as *backpropagation*. This is the fundamental process that allows us to train Neural Networks. At a top level, it involves:\n",
    "\n",
    " - Doing a feedforward operation on the network\n",
    " - Comparing the output of the model with the desired output (calculating the error)\n",
    " - Run the feedforward operation *backwards* through the network to spread the error to each of the weights\n",
    " - Use this to update the weights, and get a better model\n",
    " - Loop through this process until the model is *good enough* (based on some measure that balances accuracy and computation time)\n",
    " \n",
    "Conceptually, this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1SmY3TZTyUk?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/1SmY3TZTyUk?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the Maths behind Backpropagation can get quite complicated, as it involves multiple uses of the Chain Rule to propagate the error gradient through the network. Luckily there are a number of tools that we can use to do a lot of this for us, such as *Tensorflow*, *PyTorch* and *Keras*, amongst others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: Deriving Backpropagation\n",
    "It isn't strictly necessary to fully understand how to derive backpropagation by hand, but it can help your intuition of the process to at least see how the maths works, and understand some of the computational challenges we face when the networks grow larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tVuZDbUrzzI?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tVuZDbUrzzI?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YAhIBOnbt54?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YAhIBOnbt54?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7lidiTGIlN4?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7lidiTGIlN4?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Analyzing Student Data, Our First Neural Network\n",
    "Link to [Analyzing Student Data Lab](Labs/analyzing-student-data/StudentAdmissions.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NanoDL",
   "language": "python",
   "name": "nanodl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
